<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Mahdi Morafah</title> <meta name="author" content="Mahdi Morafah"/> <meta name="description" content="Mahdi Morafah Personal Website "/> <meta name="keywords" content="Mahdi Morafah"/> <meta property="og:site_name" content="Mahdi Morafah"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Mahdi Morafah | About"/> <meta property="og:url" content="https://mmorafah.github.io/homepage/"/> <meta property="og:description" content="Mahdi Morafah Personal Website "/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="About"/> <meta name="twitter:description" content="Mahdi Morafah Personal Website "/> <meta name="twitter:site" content="@MorafahMahdi"/> <meta name="twitter:creator" content="@MorafahMahdi"/> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Mahdi  Morafah"
        },
        "url": "https://mmorafah.github.io/homepage/",
        "@type": "WebSite",
        "description": "Mahdi Morafah Personal Website
",
        "headline": "About",
        "sameAs": ["https://scholar.google.com/citations?user=citations?user=mAtf52wAAAAJ&hl=en", "https://www.researchgate.net/profile/Mahdi_Morafah", "https://github.com/MMorafah", "https://www.linkedin.com/in/mahdi-morafah-ab97a8106", "https://twitter.com/MorafahMahdi"],
        "name": "Mahdi  Morafah",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/homepage/assets/img/ucsd_logo.png"/> <link rel="stylesheet" href="/homepage/assets/css/main.css"> <link rel="canonical" href="https://mmorafah.github.io/homepage/"> <meta name="google-site-verification" content="xzD02llqh5K8krxUJUhmeh4GnCQB55r-HL2YuSXa7k8"/> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6D%6D%6F%72%61%66%61%68@%65%6E%67.%75%63%73%64.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=citations?user=mAtf52wAAAAJ&amp;hl=en" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Mahdi_Morafah/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/MMorafah" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/mahdi-morafah-ab97a8106" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/MorafahMahdi" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/homepage/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/homepage/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/homepage/cv/">Curriculum Vitae</a> </li> <li class="nav-item "> <a class="nav-link" href="/homepage/blog/">Blog</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Mahdi Morafah </h1> <p class="desc"></p> <p class="text-success"> <font size="2"> Machine Learning Researcher <br> Generative AI and Federated Learning Enthusiast <br> Former ML Research Intern at Tesla Autopilot and Qualcomm AI </font> </p> </header> <article> <div class="profile float-right"> <figure> <picture> <source media="(max-width: 480px)" srcset="/homepage/assets/img/linkedin_photo-480.webp"></source> <source media="(max-width: 800px)" srcset="/homepage/assets/img/linkedin_photo-800.webp"></source> <source media="(max-width: 1400px)" srcset="/homepage/assets/img/linkedin_photo-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/homepage/assets/img/linkedin_photo.jpg" width="auto" height="auto" alt="linkedin_photo.jpg"> </picture> </figure> <div class="address"> <p>Department of Electrical and Computer Engineering</p> <p>University of California San Diego</p> <p>9500 Gilman Dr, La Jolla, CA 92093</p> <p> <mark> <em> Email: mmorafah 'at' ucsd 'dot' edu </em> </mark> </p> </div> </div> <div class="clearfix"> <p>I am a PhD Candidate (third year) in Electrical and Computer Engineering at University of California San Diego (UCSD), luckly being advised by <a href="http://cwcserv.ucsd.edu/~billlin/" target="_blank" rel="noopener noreferrer">Prof. Bill Lin</a>.</p> <p>I am broadly interested in Machine Learning and Optimization. My current research focus is on <strong>Federated Learning</strong>, <strong>Generative AI</strong> and <strong>Business</strong>. Here are the research themes that I am currently working on:</p> <ul> <li>Large Scale Federated Learning Over Diverse Heterogeneous Devices</li> <li>Efficient Training and Fine-tuning of Large Language Models in Federated Learning</li> <li>Fast Sampling for Diffusion Models</li> <li>Multi-Modal Foundation Models</li> <li>Effect of Generative AI in Business Market</li> </ul> <p>I previously received a M.S. in Electrical and Computer Engineering at UCSD in 2021, and a B.S. in Electrical Engineering at Tehran Polytechnique.</p> <p>I also working as a machine learning research intern at <a href="https://www.tesla.com/autopilot" target="_blank" rel="noopener noreferrer">Tesla Autopilot</a> and Qualcomm AI.</p> <p><span style="color: red;"><strong>I am looking for machine learning internship positions for summer 2024.</strong></span></p> <p><span style="color: red;"><strong>I am open to collaborate on interesting project, please feel free to reach out.</strong></span></p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Nov 20, 2023</th> <td> I am excited to successfully pass my PhD qualifying exam and being advanced to candidacy! </td> </tr> <tr> <th scope="row">Jul 20, 2023</th> <td> New accepted paper “<a href="https://ieeexplore.ieee.org/abstract/document/10189084" target="_blank" rel="noopener noreferrer">A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design</a>” [<span style="color:red"><strong>Accepted to IEEE Transactions on AI (Jul, 2023)</strong></span>]. </td> </tr> <tr> <th scope="row">May 31, 2023</th> <td> Served as Reviewer for IEEE TCNS and IEEE CDC. </td> </tr> <tr> <th scope="row">Jan 5, 2023</th> <td> Awarded AAAI 2023 student scholarship for conference attendance. </td> </tr> <tr> <th scope="row">Sep 30, 2022</th> <td> New paper on arXiv “<a href="https://arxiv.org/abs/2209.15595" target="_blank" rel="noopener noreferrer">Rethinking Data Heterogeneity in Federated Learning: Introducing a New Notion and Standard Benchmarks</a>” [<span style="color:red"><strong>Accepted to FL NeurIPS’22 Workshops (Oct, 2022)</strong></span>]. </td> </tr> <tr> <th scope="row">Sep 21, 2022</th> <td> New paper on arXiv “<a href="https://arxiv.org/abs/2209.10526" target="_blank" rel="noopener noreferrer">Efficient Distribution Similarity Identification in Clustered Federated Learning via Principal Angles Between Client Data Subspaces</a>” [<span style="color:red"><strong>Accepted to AAAI’23 (acceptance rate=19.6%) Nov, 2022</strong></span>]. </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr position-absolute"> <abbr class="badge" style="background-color: #0076df"><a href="https://aaai.org/Conferences/AAAI/aaai.php" target="_blank" rel="noopener noreferrer">AAAI</a></abbr> </div> <div> <img class="img-fluid mr-3" src="/homepage/assets/img/thumbs/vahidian2022efficient.png" alt="Generic placeholder image"> </div> </div> <div id="vahidian2022efficient" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2209.10526" target="_blank" rel="noopener noreferrer">Efficient Distribution Similarity Identification in Clustered Federated Learning via Principal Angles Between Client Data Subspaces</a></div> <div class="author"> Saeed Vahidian*, <strong>Mahdi Morafah*</strong>, Weijia Wang, Vyacheslav Kungurtsev, Chen Chen, Mubarak Shah, and Bill Lin </div> <div class="periodical"> <em>arXiv preprint arXiv:2209.10526</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2209.10526" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/MMorafah/PACFL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Clustered federated learning (FL) has been shown to produce promising results by grouping clients into clusters. This is especially effective in scenarios where separate groups of clients have significant differences in the distributions of their local data. Existing clustered FL algorithms are essentially trying to group together clients with similar distributions so that clients in the same cluster can leverage each other’s data to better perform federated learning. However, prior clustered FL algorithms attempt to learn these distribution similarities indirectly during training, which can be quite time consuming as many rounds of federated learning may be required until the formation of clusters is stabilized. In this paper, we propose a new approach to federated learning that directly aims to efficiently identify distribution similarities among clients by analyzing the principal angles between the client data subspaces. Each client applies a truncated singular value decomposition (SVD) step on its local data in a single-shot manner to derive a small set of principal vectors, which provides a signature that succinctly captures the main characteristics of the underlying distribution. This small set of principal vectors is provided to the server so that the server can directly identify distribution similarities among the clients to form clusters. This is achieved by comparing the similarities of the principal angles between the client data subspaces spanned by those principal vectors. The approach provides a simple, yet effective clustered FL framework that addresses a broad range of data heterogeneity issues beyond simpler forms of Non-IIDness like label skews. Our clustered FL approach also enables convergence guarantees for non-convex objectives. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vahidian2022efficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Distribution Similarity Identification in Clustered Federated Learning via Principal Angles Between Client Data Subspaces}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vahidian*, Saeed and Morafah, Mahdi and Wang, Weijia and Kungurtsev, Vyacheslav and Chen, Chen and Shah, Mubarak and Lin, Bill}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2209.10526}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{AAAI}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2209.10526}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MMorafah/PACFL}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">equal</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr position-absolute"> <abbr class="badge">IEEE OJCS</abbr> </div> <div> <img class="img-fluid mr-3" src="/homepage/assets/img/thumbs/morafah2022flis.png" alt="Generic placeholder image"> </div> </div> <div id="morafah2022flis" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2208.09754" target="_blank" rel="noopener noreferrer">FLIS: Clustered Federated Learning via Inference Similarity for Non-IID Data Distribution</a></div> <div class="author"> <strong>Mahdi Morafah*</strong>, Saeed Vahidian*, Weijia Wang*, and Bill Lin </div> <div class="periodical"> <em>arXiv preprint arXiv:2208.09754</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2208.09754" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/MMorafah/FLIS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Classical federated learning approaches yield significant performance degradation in the presence of Non-IID data distributions of participants. When the distribution of each local dataset is highly different from the global one, the local objective of each client will be inconsistent with the global optima which incur a drift in the local updates. This phenomenon highly impacts the performance of clients. This is while the primary incentive for clients to participate in federated learning is to obtain better personalized models. To address the above-mentioned issue, we present a new algorithm, FLIS, which groups the clients population in clusters with jointly trainable data distributions by leveraging the inference similarity of clients’ models. This framework captures settings where different groups of users have their own objectives (learning tasks) but by aggregating their data with others in the same cluster (same learning task) to perform more efficient and personalized federated learning. We present experimental results to demonstrate the benefits of FLIS over the state-of-the-art benchmarks on CIFAR-100/10, SVHN, and FMNIST datasets. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">morafah2022flis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FLIS: Clustered Federated Learning via Inference Similarity for Non-IID Data Distribution}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Morafah, Mahdi and Vahidian*, Saeed and Wang*, Weijia and Lin, Bill}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2208.09754}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IEEE OJCS}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2208.09754}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MMorafah/FLIS}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">equal</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr position-absolute"> <abbr class="badge" style="background-color: #8f911d"><a href="https://www.icdcs.org/" target="_blank" rel="noopener noreferrer">ICDCSW</a></abbr> <abbr class="badge" style="background-color: orange">Conference Award</abbr> </div> <div> <img class="img-fluid mr-3" src="/homepage/assets/img/thumbs/vahidian2021personalized.png" alt="Generic placeholder image"> </div> </div> <div id="vahidian2021personalized" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2105.00562" target="_blank" rel="noopener noreferrer">Personalized federated learning by structured and unstructured pruning under data heterogeneity</a></div> <div class="author"> Saeed Vahidian*, <strong>Mahdi Morafah*</strong>, and Bill Lin </div> <div class="periodical"> <em>In 2021 IEEE 41st International Conference on Distributed Computing Systems Workshops (ICDCSW)</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2105.00562" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/MMorafah/Sub-FedAvg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>The traditional approach in FL tries to learn a single global model collaboratively with the help of many clients under the orchestration of a central server. However, learning a single global model might not work well for all clients participating in the FL under data heterogeneity. Therefore, the personalization of the global model becomes crucial in handling the challenges that arise with statistical heterogeneity and the non-IID distribution of data. Unlike prior works, in this work we propose a new approach for obtaining a personalized model from a client-level objective. This further motivates all clients to participate in federation even under statistical het- erogeneity in order to improve their performance, instead of merely being a source of data and model training for the central server. To realize this personalization, we leverage finding a small subnetwork for each client by applying hybrid pruning (combination of structured and unstructured pruning), and unstructured pruning. Through a range of experiments on different benchmarks, we observed that the clients with similar data (labels) share similar personal parameters. By finding a subnetwork for each client rather than taking the average over all parameters of all clients for the entire federation as in traditional FL, we efficiently calculate the averaging on the remaining parameters of each subnetwork of each client. We call this novel parameter averaging as Sub-FedAvg. Furthermore, in our proposed approach, the clients are not required to have knowledge of any underlying data distributions or label similarities among the rest of clients. The non-IID nature of each client’s local data provides distinguishing subnetworks without sharing any data. We evaluate our method on federated image classification with real world datasets. Our method outperforms existing state-of-the-art.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vahidian2021personalized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Personalized federated learning by structured and unstructured pruning under data heterogeneity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vahidian*, Saeed and Morafah, Mahdi and Lin, Bill}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE 41st International Conference on Distributed Computing Systems Workshops (ICDCSW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{27--34}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICDCSW}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MMorafah/Sub-FedAvg}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2105.00562}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{Conference Award}</span><span class="p">,</span>
  <span class="na">equal</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr position-absolute"> <abbr class="badge">IEEE TCNS</abbr> </div> <div> <img class="img-fluid mr-3" src="/homepage/assets/img/thumbs/kungurtsev2021decentralized.png" alt="Generic placeholder image"> </div> </div> <div id="kungurtsev2021decentralized" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2110.10406" target="_blank" rel="noopener noreferrer">Decentralized Asynchronous Non-convex Stochastic Optimization on Directed Graphs</a></div> <div class="author"> Vyacheslav Kungurtsev, <strong>Mahdi Morafah</strong>, Tara Javidi, and Gesualdo Scutari </div> <div class="periodical"> <em>arXiv preprint arXiv:2110.10406</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2110.10406" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Distributed Optimization is an increasingly important subject area with the rise of multi-agent control and optimization. We consider a decentralized stochastic optimization problem where the agents on a graph aim to asynchronously optimize a collective (additive) objective function consisting of agents’ individual (possibly non-convex) local objective functions. Each agent only has access to a noisy estimate of the gradient of its own function (one component of the sum of objective functions). We proposed an asynchronous distributed algorithm for such a class of problems. The algorithm combines stochastic gradients with tracking in an asynchronous push-sum framework and obtain the standard sublinear convergence rate for general non-convex functions, matching the rate of centralized stochastic gradient descent SGD. Our experiments on a non-convex image classification task using convolutional neural network validate the convergence of our proposed algorithm across different number of nodes and graph connectivity percentages.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr position-absolute"> <abbr class="badge" style="background-color: #69438c"><a href="http://www.nips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a></abbr> </div> <div> <img class="img-fluid mr-3" src="/homepage/assets/img/thumbs/morafah2022rethinking.png" alt="Generic placeholder image"> </div> </div> <div id="morafah2022rethinking" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2209.15595" target="_blank" rel="noopener noreferrer">Rethinking Data Heterogeneity in Federated Learning: Introducing a New Notion and Standard Benchmarks</a></div> <div class="author"> <strong>Mahdi Morafah*</strong>, Saeed Vahidian*, Chen Chen, Mubarak Shah, and Bill Lin </div> <div class="periodical"> <em>arXiv preprint arXiv:2209.15595</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2209.15595" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/MMorafah/FL-SC-NIID" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p> Though successful, federated learning presents new challenges for machine learning, especially when the issue of data heterogeneity, also known as Non-IID data, arises. To cope with the statistical heterogeneity, previous works incorporated a proximal term in local optimization or modified the model aggregation scheme at the server side or advocated clustered federated learning approaches where the central server groups agent population into clusters with jointly trainable data distributions to take the advantage of a certain level of personalization. While effective, they lack a deep elaboration on what kind of data heterogeneity and how the data heterogeneity impacts the accuracy performance of the participating clients. In contrast to many of the prior federated learning approaches, we demonstrate not only the issue of data heterogeneity in current setups is not necessarily a problem but also in fact it can be beneficial for the FL participants. Our observations are intuitive: (1) Dissimilar labels of clients (label skew) are not necessarily considered data heterogeneity, and (2) the principal angle between the agents’ data subspaces spanned by their corresponding principal vectors of data is a better estimate of the data heterogeneity.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">morafah2022rethinking</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rethinking Data Heterogeneity in Federated Learning: Introducing a New Notion and Standard Benchmarks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Morafah, Mahdi and Vahidian*, Saeed and Chen, Chen and Shah, Mubarak and Lin, Bill}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2209.15595}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2209.15595}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MMorafah/FL-SC-NIID}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">equal</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr position-absolute"> <abbr class="badge">IEEE TAI</abbr> </div> <div> <img class="img-fluid mr-3" src="/homepage/assets/img/thumbs/morafah2023practical.png" alt="Generic placeholder image"> </div> </div> <div id="morafah2023practical" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2307.15245" target="_blank" rel="noopener noreferrer">A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design</a></div> <div class="author"> <strong>Mahdi Morafah</strong>, Weijia Wang, and Bill Lin </div> <div class="periodical"> <em>IEEE Transactions on Artificial Intelligence</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.15245" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/MMorafah/FedZoo-Bench" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench, an open-source library based on PyTorch with pre-implementation of 22 state-of-the-art methods, and a broad set of standardized and customizable features. We also provide a comprehensive comparison of several state-of-the-art (SOTA) methods to better understand the current state of the field and existing limitations. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">morafah2023practical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Morafah, Mahdi and Wang, Weijia and Lin, Bill}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IEEE TAI}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2307.15245}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MMorafah/FedZoo-Bench}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Mahdi Morafah. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: January 11, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/homepage/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/homepage/assets/js/zoom.js"></script> <script defer src="/homepage/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>