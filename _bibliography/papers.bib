---
---

@article{vahidian2022efficient,
  title={Efficient Distribution Similarity Identification in Clustered Federated Learning via Principal Angles Between Client Data Subspaces},
  author={Vahidian*, Saeed and Morafah, Mahdi and Wang, Weijia and Kungurtsev, Vyacheslav and Chen, Chen and Shah, Mubarak and Lin, Bill},
  journal={arXiv preprint arXiv:2209.10526},
  year={2022},
  abbr={AAAI},
  arxiv={2209.10526},
  bibtex_show={true}, 
  code={https://github.com/MMorafah/PACFL},
  selected={true},
  equal={true}
}

@article{morafah2022flis,
  title={FLIS: Clustered Federated Learning via Inference Similarity for Non-IID Data Distribution},
  author={Morafah, Mahdi and Vahidian*, Saeed and Wang*, Weijia and Lin, Bill},
  journal={arXiv preprint arXiv:2208.09754},
  year={2022},
  abbr={NeurIPS},
  abbr={IEEE OJCS},
  arxiv={2208.09754},
  bibtex_show={true}, 
  code={https://github.com/MMorafah/FLIS},
  selected={true},
  equal={true}
}

@inproceedings{vahidian2021personalized,
  title={Personalized federated learning by structured and unstructured pruning under data heterogeneity},
  author={Vahidian*, Saeed and Morafah, Mahdi and Lin, Bill},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems Workshops (ICDCSW)},
  pages={27--34},
  year={2021},
  organization={IEEE}, 
  bibtex_show={true}, 
  abbr={ICDCSW}, 
  code={https://github.com/MMorafah/Sub-FedAvg},
  arxiv={2105.00562},
  abstract={The traditional approach in FL tries to learn a single global model collaboratively
            with the help of many clients under the orchestration of a central server. However,
            learning a single global model might not work well for all clients participating in
            the FL under data heterogeneity. Therefore, the personalization of the global model
            becomes crucial in handling the challenges that arise with statistical heterogeneity
            and the non-IID distribution of data. Unlike prior works, in this work we propose a
            new approach for obtaining a personalized model from a client-level objective. This
            further motivates all clients to participate in federation even under statistical het-
            erogeneity in order to improve their performance, instead of merely being a source
            of data and model training for the central server. To realize this personalization,
            we leverage finding a small subnetwork for each client by applying hybrid pruning
            (combination of structured and unstructured pruning), and unstructured pruning.
            Through a range of experiments on different benchmarks, we observed that the
            clients with similar data (labels) share similar personal parameters. By finding a
            subnetwork for each client rather than taking the average over all parameters of
            all clients for the entire federation as in traditional FL, we efficiently calculate the
            averaging on the remaining parameters of each subnetwork of each client. We
            call this novel parameter averaging as Sub-FedAvg. Furthermore, in our proposed
            approach, the clients are not required to have knowledge of any underlying data
            distributions or label similarities among the rest of clients. The non-IID nature of
            each clientâ€™s local data provides distinguishing subnetworks without sharing any
            data. We evaluate our method on federated image classification with real world
            datasets. Our method outperforms existing state-of-the-art.}, 
  
  award={Conference Award},
  equal={true},
  selected={true}
}
@article{kungurtsev2021decentralized,
  title={Decentralized Asynchronous Non-convex Stochastic Optimization on Directed Graphs},
  author={Kungurtsev, Vyacheslav and Morafah, Mahdi and Javidi, Tara and Scutari, Gesualdo},
  journal={arXiv preprint arXiv:2110.10406},
  year={2021},
  abbr={IEEE TCNS},
  arxiv={2110.10406}, 
  selected={true}
}

@article{morafah2022rethinking,
  title={Rethinking Data Heterogeneity in Federated Learning: Introducing a New Notion and Standard Benchmarks},
  author={Morafah, Mahdi and Vahidian*, Saeed and Chen, Chen and Shah, Mubarak and Lin, Bill},
  journal={arXiv preprint arXiv:2209.15595},
  year={2022},
  abbr={NeurIPS},
  arxiv={2209.15595},
  bibtex_show={true}, 
  code={https://github.com/MMorafah/FL-SC-NIID},
  selected={true},
  equal={true}
}

@article{morafah2023practical,
  title={A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design},
  author={Morafah, Mahdi and Wang, Weijia and Lin, Bill},
  journal={IEEE Transactions on Artificial Intelligence},
  year={2023},
  abbr={IEEE TAI},
  organization={IEEE},
  arxiv={2307.15245},
  bibtex_show={true}, 
  code={https://github.com/MMorafah/FedZoo-Bench},
  abstract={Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench, an open-source library based on PyTorch with pre-implementation of 22 state-of-the-art methods, and a broad set of standardized and customizable features. We also provide a comprehensive comparison of several state-of-the-art (SOTA) methods to better understand the current state of the field and existing limitations. },
  selected={true}
}
